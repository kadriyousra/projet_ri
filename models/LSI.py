import numpy as np
from collections import defaultdict
import os
import sys
from typing import List, Dict, Tuple


class LSIModel:
    """Latent Semantic Indexing (LSI) Model"""
    
    def __init__(self, k: int = 100):
        """
        Args:
            k: Nombre de dimensions latentes (rÃ©duction de dimensionnalitÃ©)
        """
        self.k = k
        
        # Vocabulaire et documents
        self.vocabulary = []      # Liste ordonnÃ©e des termes
        self.doc_ids = []         # Liste ordonnÃ©e des doc_ids
        
        # Matrices SVD
        self.W = None             # Matrice TF-IDF (M Ã— N)
        self.U = None             # Matrice des termes (M Ã— min(M,N))
        self.S = None             # Valeurs singuliÃ¨res (diagonale)
        self.VT = None            # Matrice des documents transposÃ©e (min(M,N) Ã— N)
        
        # Matrices rÃ©duites (k dimensions)
        self.Uk = None            # U tronquÃ©e (M Ã— k)
        self.Sk = None            # S tronquÃ©e (k Ã— k)
        self.VTk = None           # VT tronquÃ©e (k Ã— N)
        
        # Matrice de projection des requÃªtes
        self.M = None             # M = Uk @ Sk^-1 (M Ã— k)
        
        # Cache pour SÂ² @ D (formule exacte du code 2)
        self.S2_D = None          # (Sk @ Sk) @ VTk
    
    
    def load_inverted_index(self, filepath: str, verbose: bool = True):
        """Charge l'inverted index et construit la matrice TF-IDF"""
        if verbose:
            print("\n" + "="*80)
            print("Ã‰TAPE 1: CHARGEMENT DE L'INVERTED INDEX")
            print("="*80)
        
        # Structure: {term: {doc_id: weight}}
        term_doc_weights = defaultdict(dict)
        all_docs = set()
        all_terms = set()
        
        # Lire le fichier
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                
                parts = line.split()
                if len(parts) >= 4:
                    term = parts[0]
                    doc_id = int(parts[1])
                    weight = float(parts[3])  # TF-IDF weight
                    
                    term_doc_weights[term][doc_id] = weight
                    all_terms.add(term)
                    all_docs.add(doc_id)
        
        # CrÃ©er les listes ordonnÃ©es
        self.vocabulary = sorted(list(all_terms))
        self.doc_ids = sorted(list(all_docs))
        
        M = len(self.vocabulary)  # Nombre de termes
        N = len(self.doc_ids)     # Nombre de documents
        
        if verbose:
            print(f"\nğŸ“Š Statistiques:")
            print(f"   - Termes dans le vocabulaire: {M}")
            print(f"   - Documents: {N}")
        
        # Construire la matrice TF-IDF W (M Ã— N)
        self.W = np.zeros((M, N), dtype=np.float32)
        
        for i, term in enumerate(self.vocabulary):
            for j, doc_id in enumerate(self.doc_ids):
                if doc_id in term_doc_weights[term]:
                    self.W[i, j] = term_doc_weights[term][doc_id]
        
        if verbose:
            non_zero = np.count_nonzero(self.W)
            density = (non_zero / (M * N)) * 100
            print(f"\nâœ… Matrice W construite: {M} Ã— {N}")
            print(f"   - Ã‰lÃ©ments non-nuls: {non_zero:,}")
            print(f"   - DensitÃ©: {density:.2f}%")
            print("="*80)
    
    
    def apply_svd(self, verbose: bool = True):
        """Applique la dÃ©composition SVD sur la matrice TF-IDF"""
        if verbose:
            print("\n" + "="*80)
            print("Ã‰TAPE 2: DÃ‰COMPOSITION SVD")
            print("="*80)
        
        self.U, s, self.VT = np.linalg.svd(self.W, full_matrices=False)
        self.S = np.diag(s)
        
        if verbose:
            print(f"\nğŸ“ Dimensions des matrices:")
            print(f"   - U:  {self.U.shape}")
            print(f"   - S:  {self.S.shape}")
            print(f"   - VT: {self.VT.shape}")
            print(f"\nğŸ“Š Top 10 valeurs singuliÃ¨res:")
            for i, val in enumerate(s[:10], 1):
                print(f"   Ïƒ{i}: {val:.4f}")
            
            # Variance expliquÃ©e
            total_variance = np.sum(s**2)
            cumsum = np.cumsum(s**2)
            variance_k = (cumsum[self.k-1] / total_variance) * 100
            print(f"\nğŸ’¡ Variance expliquÃ©e par k={self.k}: {variance_k:.2f}%")
            print("="*80)
    
    
    def reduce_dimensionality(self, verbose: bool = True):
        """RÃ©duit la dimensionnalitÃ© Ã  k dimensions"""
        if verbose:
            print("\n" + "="*80)
            print(f"Ã‰TAPE 3: RÃ‰DUCTION Ã€ k={self.k} DIMENSIONS")
            print("="*80)
        
        # Tronquer aux k premiÃ¨res dimensions
        self.Uk = self.U[:, :self.k]
        self.Sk = self.S[:self.k, :self.k]
        self.VTk = self.VT[:self.k, :]
        
        if verbose:
            print(f"\nğŸ“ Matrices rÃ©duites:")
            print(f"   - Uk:  {self.Uk.shape}")
            print(f"   - Sk:  {self.Sk.shape}")
            print(f"   - VTk: {self.VTk.shape}")
        
        # Calculer la matrice de projection M = Uk @ Sk^-1
        Sk_inv = np.linalg.inv(self.Sk)
        self.M = self.Uk @ Sk_inv
        
        # PrÃ©calculer SÂ² @ D pour la similaritÃ©
        self.S2_D = (self.Sk @ self.Sk) @ self.VTk
        
        if verbose:
            print(f"\nâœ… Matrice de projection M: {self.M.shape}")
            print(f"âœ… SÂ² @ D prÃ©calculÃ©: {self.S2_D.shape}")
            print("="*80)
    
    
    def project_query(self, query_terms: List[str]) -> np.ndarray:
        """Projette une requÃªte dans l'espace latent rÃ©duit"""
        # CrÃ©er le vecteur de requÃªte dans l'espace original
        q = np.zeros(len(self.vocabulary), dtype=np.float32)
        
        # Marquer les termes prÃ©sents dans la requÃªte (prÃ©sence binaire)
        found_terms = []
        for term in query_terms:
            if term in self.vocabulary:
                idx = self.vocabulary.index(term)
                q[idx] = 1.0
                found_terms.append(term)
        
        if len(found_terms) == 0:
            # Aucun terme de la requÃªte n'existe dans le vocabulaire
            return None
        
        # Projection: q_new = q^T @ M
        q_new = q.T @ self.M  # Shape: (k,)
        
        return q_new
    
    
    def calculate_similarity(self, q_new: np.ndarray) -> np.ndarray:
        """Calcule les similaritÃ©s entre la requÃªte projetÃ©e et tous les documents"""
        # SimilaritÃ©: sim = q_new @ (SÂ² @ D)
        sim = q_new @ self.S2_D
        
        return sim
    
    
    def rank_documents(self, query_terms: List[str], top_k: int = None) -> List[Tuple[int, float]]:
        """
        Classe tous les documents pour une requÃªte
        
        Args:
            query_terms: Liste des termes de la requÃªte
            top_k: Si spÃ©cifiÃ©, limite le nombre de rÃ©sultats (pour affichage)
                   Si None, retourne TOUS les documents (requis pour Ã©valuation)
        
        Returns:
            Liste de tuples (doc_id, score) triÃ©e par score dÃ©croissant
        """
        # Projeter la requÃªte
        q_new = self.project_query(query_terms)
        
        if q_new is None:
            # Aucun terme trouvÃ©, retourner tous les docs avec score 0
            return [(doc_id, 0.0) for doc_id in self.doc_ids]
        
        # Calculer les similaritÃ©s
        similarities = self.calculate_similarity(q_new)
        
        # CrÃ©er la liste (doc_id, score)
        doc_scores = [(self.doc_ids[i], float(similarities[i])) 
                     for i in range(len(self.doc_ids))]
        
        # Trier par score dÃ©croissant
        doc_scores.sort(key=lambda x: x[1], reverse=True)
        
        # Retourner top_k si spÃ©cifiÃ© (pour affichage uniquement)
        if top_k is not None:
            doc_scores = doc_scores[:top_k]
        
        return doc_scores
    
    
    def fit(self, inverted_index_path: str, verbose: bool = True):
        """
        EntraÃ®ne le modÃ¨le LSI
        
        Args:
            inverted_index_path: Chemin vers l'inverted index
            verbose: Afficher les dÃ©tails
        """
        if verbose:
            print("\n" + "="*80)
            print("ENTRAÃNEMENT DU MODÃˆLE LSI")
            print("="*80)
            print(f"ParamÃ¨tre k: {self.k}")
        
        # Ã‰tape 1: Charger l'inverted index
        self.load_inverted_index(inverted_index_path, verbose)
        
        # Ã‰tape 2: Appliquer SVD
        self.apply_svd(verbose)
        
        # Ã‰tape 3: RÃ©duire la dimensionnalitÃ©
        self.reduce_dimensionality(verbose)
        
        if verbose:
            print("\n" + "="*80)
            print("âœ… MODÃˆLE LSI PRÃŠT")
            print("="*80)
    
    
    def search(self, query_terms: List[str], top_k: int = 10, 
              verbose: bool = False, return_all: bool = False) -> List[int]:
        """
        Recherche les documents pertinents pour une requÃªte
        
        Args:
            query_terms: Liste des termes de la requÃªte
            top_k: Nombre de documents Ã  retourner (ignorÃ© si return_all=True)
            verbose: Afficher les rÃ©sultats
            return_all: Si True, retourne TOUS les documents (pour Ã©valuation)
        
        Returns:
            Liste des doc_ids classÃ©s par pertinence
        """
        # Obtenir tous les documents classÃ©s
        doc_scores = self.rank_documents(query_terms, top_k=None)
        
        if verbose and doc_scores:
            display_k = min(top_k, len(doc_scores))
            print(f"\nğŸ” Top {display_k} documents:")
            print(f"{'Rang':<6} {'Doc ID':<10} {'Score':<12}")
            print("-" * 30)
            for rank, (doc_id, score) in enumerate(doc_scores[:display_k], 1):
                print(f"{rank:<6} {doc_id:<10} {score:.6f}")
        
        # Extraire les doc_ids
        ranked_list = [doc_id for doc_id, score in doc_scores]
        
        # Limiter seulement si demandÃ© ET pas return_all
        if top_k and not return_all:
            ranked_list = ranked_list[:top_k]
        
        return ranked_list


# ============================================================================
# Ã‰VALUATION COMPLÃˆTE AVEC METRICS.PY
# ============================================================================

if __name__ == "__main__":
    
    # Ajouter le dossier src et evaluation au path
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_dir = os.path.dirname(current_dir)
    src_dir = os.path.join(project_dir, 'src')
    eval_dir = os.path.join(project_dir, 'evaluation')
    sys.path.insert(0, src_dir)
    sys.path.insert(0, eval_dir)
    
    from medline_parser import parse_med_qry, parse_med_rel
    from preprocessing import MEDLINEPreprocessor
    from metrics import IRMetrics
    
    # Chemins
    INVERTED_INDEX_PATH = os.path.join(project_dir, "data", "ouput", "inverted_index.txt")
    MED_QRY_PATH = os.path.join(project_dir, "data", "MED.QRY")
    MED_REL_PATH = os.path.join(project_dir, "data", "MED.REL")
    
    # VÃ©rifier les fichiers
    for path, name in [(INVERTED_INDEX_PATH, "Inverted Index"),
                       (MED_QRY_PATH, "MED.QRY"),
                       (MED_REL_PATH, "MED.REL")]:
        if not os.path.exists(path):
            print(f"âŒ ERREUR: Fichier non trouvÃ©: {path}")
            exit(1)
    
    print("="*80)
    print("Ã‰VALUATION COMPLÃˆTE DU MODÃˆLE LSI")
    print("="*80)
    
    # 1. CrÃ©er et entraÃ®ner le modÃ¨le
    print("\nğŸ“š Ã‰tape 1: EntraÃ®nement du modÃ¨le LSI")
    lsi = LSIModel(k=100)
    lsi.fit(INVERTED_INDEX_PATH, verbose=True)
    
    # 2. Charger les donnÃ©es
    print("\nğŸ“„ Ã‰tape 2: Chargement des donnÃ©es")
    queries = parse_med_qry(MED_QRY_PATH)
    relevance_judgments = parse_med_rel(MED_REL_PATH)
    print(f"âœ… {len(queries)} requÃªtes chargÃ©es")
    print(f"âœ… {len(relevance_judgments)} jugements de pertinence chargÃ©s")
    
    # 3. CrÃ©er le preprocessor
    preprocessor = MEDLINEPreprocessor()
    
    # 4. Initialiser le systÃ¨me de mÃ©triques
    print("\nğŸ“Š Ã‰tape 3: Initialisation du systÃ¨me d'Ã©valuation")
    metrics = IRMetrics(relevance_judgments, model_name="LSI_k100")
    
    # 5. Collecter tous les rÃ©sultats
    print("\nğŸ” Ã‰tape 4: Traitement de toutes les requÃªtes")
    results_per_query = {}
    relevance_scores_per_query = {}
    
    for query in queries:
        query_id = query.query_id
        query_text = query.text
        
        # Preprocesser la requÃªte
        query_terms = preprocessor.preprocess_text(query_text)
        
        # âœ… CRITICAL: Obtenir TOUS les documents classÃ©s (pas de limitation top_k)
        doc_scores = lsi.rank_documents(query_terms, top_k=None)
        
        # Extraire les doc_ids et les scores
        ranked_list = [doc_id for doc_id, score in doc_scores]
        scores_dict = {doc_id: score for doc_id, score in doc_scores}
        
        results_per_query[query_id] = ranked_list
        relevance_scores_per_query[query_id] = scores_dict
        
        print(f"   RequÃªte {query_id}: {len(ranked_list)} documents classÃ©s")
    
    # 6. Ã‰valuer le systÃ¨me complet
    print("\nğŸ“ˆ Ã‰tape 5: Ã‰valuation complÃ¨te du systÃ¨me")
    all_results = metrics.evaluate_all_queries(
        results_per_query=results_per_query,
        relevance_scores_per_query=relevance_scores_per_query,  # Pour DCG/nDCG
        plot_curves=True,
        save_results=True,
        verbose=False  # Mettre True pour voir les dÃ©tails de chaque requÃªte
    )
    
    print("\n" + "="*80)
    print("âœ… Ã‰VALUATION TERMINÃ‰E")
    print("="*80)
    print(f"ğŸ“ RÃ©sultats sauvegardÃ©s:")
    print(f"   - results/LSI_k100_results.txt")
    print(f"   - results/figures/LSI_k100/")
    print("="*80)