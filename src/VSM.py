import numpy as np
from collections import defaultdict
import os
from typing import List, Dict, Tuple


class VSMModel:
    
    
    def __init__(self):
        # Vocabulaire et documents
        self.vocabulary = []      # Liste ordonnÃ©e des termes
        self.doc_ids = []         # Liste ordonnÃ©e des doc_ids
        
        # Matrice TF-IDF (M Ã— N)
        self.doc_vectors = None   # Vecteurs TF-IDF des documents
        
        # Index inversÃ© pour accÃ¨s rapide
        self.inverted_index = {}  # {term: {doc_id: tfidf_weight}}
    
    
    def load_inverted_index(self, filepath: str, verbose: bool = True):
       
        if verbose:
            print("\n" + "="*80)
            print("CHARGEMENT DE L'INVERTED INDEX")
            print("="*80)
        
        # Structure: {term: {doc_id: weight}}
        term_doc_weights = defaultdict(dict)
        all_docs = set()
        all_terms = set()
        
        # Lire le fichier
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                
                parts = line.split()
                if len(parts) >= 4:
                    term = parts[0]
                    doc_id = int(parts[1])  # Convertir en int pour tri
                    weight = float(parts[3])  # TF-IDF weight
                    
                    term_doc_weights[term][doc_id] = weight
                    all_terms.add(term)
                    all_docs.add(doc_id)
        
        # CrÃ©er les listes ordonnÃ©es
        self.vocabulary = sorted(list(all_terms))
        self.doc_ids = sorted(list(all_docs))
        
        M = len(self.vocabulary)  # Nombre de termes
        N = len(self.doc_ids)     # Nombre de documents
        
        if verbose:
            print(f"\nğŸ“Š Statistiques:")
            print(f"   - Termes dans le vocabulaire: {M}")
            print(f"   - Documents: {N}")
        
        # Construire la matrice TF-IDF (M Ã— N)
        self.doc_vectors = np.zeros((M, N), dtype=np.float32)
        
        for i, term in enumerate(self.vocabulary):
            for j, doc_id in enumerate(self.doc_ids):
                if doc_id in term_doc_weights[term]:
                    self.doc_vectors[i, j] = term_doc_weights[term][doc_id]
        
        # Stocker l'index inversÃ©
        self.inverted_index = {term: dict(term_doc_weights[term]) 
                              for term in self.vocabulary}
        
        if verbose:
            non_zero = np.count_nonzero(self.doc_vectors)
            density = (non_zero / (M * N)) * 100
            print(f"\nâœ… Matrice document-terme construite: {M} Ã— {N}")
            print(f"   - Ã‰lÃ©ments non-nuls: {non_zero:,}")
            print(f"   - DensitÃ©: {density:.2f}%")
            print("="*80)
    
    
    def create_query_vector(self, query_terms: List[str]) -> np.ndarray:
        
        # CrÃ©er le vecteur de requÃªte (prÃ©sence binaire)
        query_vector = np.zeros(len(self.vocabulary), dtype=np.float32)
        
        found_terms = []
        for term in query_terms:
            if term in self.vocabulary:
                idx = self.vocabulary.index(term)
                query_vector[idx] = 1.0  # PondÃ©ration binaire
                found_terms.append(term)
        
        if len(found_terms) == 0:
            return None
        
        return query_vector
    
    
    def compute_cosine_similarity(self, query_vector: np.ndarray) -> np.ndarray:
       
        # Produit scalaire query Â· documents
        dot_products = query_vector @ self.doc_vectors  # (N,)
        
        # Norme de la requÃªte
        query_norm = np.linalg.norm(query_vector)
        if query_norm == 0:
            return np.zeros(len(self.doc_ids))
        
        # Normes des documents
        doc_norms = np.linalg.norm(self.doc_vectors, axis=0)  # (N,)
        
        # Ã‰viter division par zÃ©ro
        doc_norms[doc_norms == 0] = 1
        
        # SimilaritÃ© cosinus
        similarities = dot_products / (query_norm * doc_norms)
        
        return similarities
    
    
    def rank_documents(self, query_terms: List[str], top_k: int = None) -> List[Tuple[int, float]]:
        
        # CrÃ©er le vecteur de requÃªte
        query_vector = self.create_query_vector(query_terms)
        
        if query_vector is None:
            # Aucun terme trouvÃ©
            return []
        
        # Calculer les similaritÃ©s cosinus
        similarities = self.compute_cosine_similarity(query_vector)
        
        # CrÃ©er la liste (doc_id, score)
        doc_scores = [(self.doc_ids[i], similarities[i]) 
                     for i in range(len(self.doc_ids))]
        
        # Trier par score dÃ©croissant
        doc_scores.sort(key=lambda x: x[1], reverse=True)
        
        # Retourner top_k si spÃ©cifiÃ©
        if top_k is not None:
            doc_scores = doc_scores[:top_k]
        
        return doc_scores
    
    
    def fit(self, inverted_index_path: str, verbose: bool = True):
       
        if verbose:
            print("\n" + "="*80)
            print("INITIALISATION DU MODÃˆLE VSM")
            print("="*80)
            print("Mesure de similaritÃ©: Cosine")
        
        # Charger l'inverted index
        self.load_inverted_index(inverted_index_path, verbose)
        
        if verbose:
            print("\n" + "="*80)
            print("âœ… MODÃˆLE VSM PRÃŠT")
            print("="*80)
    
    
    def search(self, query_terms: List[str], top_k: int = 10, verbose: bool = False) -> List[int]:
        
        doc_scores = self.rank_documents(query_terms, top_k=top_k)
        
        if verbose and doc_scores:
            print(f"\nğŸ” Top {min(top_k, len(doc_scores))} documents:")
            print(f"{'Rang':<6} {'Doc ID':<10} {'Score':<12}")
            print("-" * 30)
            for rank, (doc_id, score) in enumerate(doc_scores[:top_k], 1):
                print(f"{rank:<6} {doc_id:<10} {score:.6f}")
        
        # Retourner seulement les doc_ids
        return [doc_id for doc_id, score in doc_scores]


# ============================================================================
# TEST SUR TOUTES LES REQUÃŠTES MED.QRY
# ============================================================================

if __name__ == "__main__":
    
    from medline_parser import parse_med_qry
    from preprocessing import MEDLINEPreprocessor
    
    # Chemins
    INVERTED_INDEX_PATH = r"C:\Users\pc\Desktop\RI_Project\data\output\inverted_index.txt"
    MED_QRY_PATH = r"C:\Users\pc\Desktop\RI_Project\data\MED.QRY"
    
    # VÃ©rifier les fichiers
    if not os.path.exists(INVERTED_INDEX_PATH):
        print(f"âŒ ERREUR: Fichier non trouvÃ©: {INVERTED_INDEX_PATH}")
        print("\nğŸ’¡ Assurez-vous d'avoir exÃ©cutÃ© preprocessing.py pour gÃ©nÃ©rer l'inverted index")
        exit(1)
    
    if not os.path.exists(MED_QRY_PATH):
        print(f"âŒ ERREUR: Fichier non trouvÃ©: {MED_QRY_PATH}")
        exit(1)
    
    # CrÃ©er et charger le modÃ¨le
    print("="*80)
    print("TEST DU MODÃˆLE VSM SUR TOUTES LES REQUÃŠTES MED.QRY")
    print("="*80)
    
    vsm = VSMModel()
    vsm.fit(INVERTED_INDEX_PATH, verbose=True)
    
    # Charger les requÃªtes
    print("\nğŸ“„ Chargement des requÃªtes...")
    queries = parse_med_qry(MED_QRY_PATH)
    print(f"âœ… {len(queries)} requÃªtes chargÃ©es")
    
    # CrÃ©er le preprocessor
    preprocessor = MEDLINEPreprocessor()
    
    # Tester chaque requÃªte
    print("\n" + "="*80)
    print("TRAITEMENT DES REQUÃŠTES")
    print("="*80)
    
    for query in queries:
        query_id = query.query_id
        query_text = query.text
        
        # Preprocesser la requÃªte
        query_terms = preprocessor.preprocess_text(query_text)
        
        print(f"\n{'='*80}")
        print(f"ğŸ“ RequÃªte {query_id}")
        print(f"{'='*80}")
        print(f"Texte: {query_text[:100]}...")
        print(f"Termes preprocessÃ©s: {query_terms[:10]}...")
        
        # Rechercher les documents
        results = vsm.search(query_terms, top_k=10, verbose=True)
        
        if not results:
            print("âš ï¸  Aucun rÃ©sultat trouvÃ© pour cette requÃªte")
    
    print("\n" + "="*80)
    print("âœ… TEST TERMINÃ‰ SUR TOUTES LES REQUÃŠTES")
    print("="*80)